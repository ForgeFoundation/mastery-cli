
const aws_certification_associate_developer = [
    {
        prompt: "A startup with newly created AWS account is testing different EC2 instances. They have used Burstable performance instance - T2.micro - for 35 seconds and stopped the instance.  At the end of the month, what is the instance usage duration that the company is charged for?",
        example: "0 seconds",
        term: "T2.micro",
    },
    {
        prompt: "An application is hosted by a 3rd party and exposed at yourapp.3rdparty.com. You would like to have your users access your application using www.mydomain.com, which you own and manage under Route 53.  What Route 53 record should you create?",
        example: "A CNAME record\n\n\
        A CNAME record maps DNS queries for the name of the current record, such as acme.example.com, to another domain (example.com or example.net) or subdomain (acme.example.com or zenith.example.org).  CNAME records can be used to map one domain name to another. Although you should keep in mind that the DNS protocol does not allow you to create a CNAME record for the top node of a DNS namespace, also known as the zone apex. For example, if you register the DNS name example.com, the zone apex is example.com. You cannot create a CNAME record for example.com, but you can create CNAME records for www.example.com, newproduct.example.com, and so on.",
        term: "domain name system records", 
        description: "A Record (Address Record):  An A record is a type of DNS (Domain Name System) record that maps a domain name to an IPv4 address. In other words, an A record is used to translate a human-readable domain name (such as www.example.com) into a machine-readable IP address (such as 192.0.2.1).  Alias Record (ANAME or ALIAS):  Alias records are DNS records that allow a DNS query for a domain name to be redirected to another domain name. An alias record is similar to a CNAME record, but it allows the root domain to be used in the DNS query. Alias records are useful for pointing a domain name to a service that is hosted on another domain name, such as pointing a subdomain to a load balancer.  CNAME (Canonical Name) Record:  A CNAME record is a type of DNS record that maps one domain name to another. This is useful for creating aliases for a domain or for pointing a subdomain to another domain. For example, if you have a subdomain called shop.example.com and you want it to point to another domain called store.example.net, you can create a CNAME record that maps shop.example.com to store.example.net.  PTR (Pointer) Record:  A PTR record is a type of DNS record that maps an IP address to a domain name. PTR records are used in reverse DNS (rDNS) lookups to determine the domain name associated with a given IP address. This is useful for verifying the identity of a server or for troubleshooting network issues."
    },
    {
        prompt: "A developer has been asked to create an application that can be deployed across a fleet of EC2 instances. The configuration must allow for full control over the deployment steps using the blue-green deployment.  Which service will help you achieve that?",
        example: "AWS CodeDeploy\n\n\
        AWS CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, or serverless Lambda functions. AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy.  The blue/green deployment type uses the blue/green deployment model controlled by CodeDeploy. This deployment type enables you to verify a new deployment of service before sending production traffic to it.",
        term: "AWS blue  green deployment",
        description: `Blue-green deployment is a deployment strategy that involves creating two identical environments, one "blue" and one "green," where only one of the environments is live at a time.  In this deployment model, the current production environment, or "blue" environment, remains active and serving traffic while a new "green" environment is deployed with the latest code changes and tested to ensure it is functioning properly. Once the "green" environment has been verified and is ready to go live, traffic is routed to it while the "blue" environment is taken down and updated with any necessary changes.  The process can be repeated in the future, with the "blue" and "green" environments swapping roles. This deployment approach reduces the risk of downtime or errors during the deployment process by enabling the new version to be fully tested before it goes live, and allowing for quick rollbacks if issues arise. It also provides a way to achieve zero-downtime deployments, as the switch from "blue" to "green" can be done seamlessly without impacting end-users.\n\
        AWS CodeDeploy is designed to complement other AWS deployment services like AWS CodePipeline, AWS CodeBuild, and AWS Elastic Beanstalk, and offers unique advantages in certain scenarios.  While AWS CodePipeline and AWS CodeBuild can be used to automate the build, test, and deployment process, they don't offer the same level of control and customization over the deployment process as AWS CodeDeploy. AWS CodeDeploy provides more granular control over the deployment process and can handle more complex deployment scenarios, such as blue-green deployments and canary releases.  AWS Elastic Beanstalk, on the other hand, is a fully managed platform that automatically handles the deployment and management of applications, but it provides less control over the deployment process and requires the use of specific platforms and configurations.  Overall, AWS CodeDeploy offers advantages in scenarios where a higher degree of control and customization over the deployment process is required, and where more complex deployment scenarios are needed, such as when deploying to on-premises servers, Lambda functions, or other compute services that are not supported by AWS Elastic Beanstalk.`
    },
    {
        prompt: "You are a developer working on AWS Lambda functions that are invoked via REST API's using Amazon API Gateway. Currently, when a GET request is invoked by the consumer, the entire data-set returned by the Lambda function is visible. Your team lead asked you to format the data response.  Which feature of the API Gateway can be used to solve this issue?",
        example: "Mapping Templates\n\n\
        Use API Gateway Mapping Templates - In API Gateway, an API's method request can take a payload in a different format from the corresponding integration request payload, as required in the backend. Similarly, vice versa is also possible. API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response.",
        term: "mapping reponses"
    },
    {
        prompt: "A developer has an application that stores data in an Amazon S3 bucket. The application uses an HTTP API to store and retrieve objects. When the PutObject API operation adds objects to the S3 bucket the developer must encrypt these objects at rest by using server-side encryption with Amazon S3-managed keys (SSE-S3).  Which solution will guarantee that any upload request without the mandated encryption is not processed?",
        example: "Invoke the PutObject API operation and set the x-amz-server-side-encryption header as AES256. Use an S3 bucket policy to deny permission to upload an object unless the request has this header",
        description: ':m SSE-S3 server-side encryption protects data at rest. Amazon S3 encrypts each object with a unique key. As an additional safeguard, it encrypts the key itself with a key that it rotates regularly. Amazon S3 server-side encryption uses one of the strongest block ciphers available to encrypt your data, 256-bit Advanced Encryption Standard (AES-256).  You can use the following bucket policy to deny permissions to upload an object unless the request includes the x-amz-server-side-encryption header to request server-side encryption using SSE-S3:\n\n\
        ```json\n\
        {\n "Version": "2012-10-17",\n "Id": "PutObjectPolicy",\n "Statement": [\n {\n "Sid": "DenyIncorrectEncryptionHeader",\n "Effect": "Deny",\n "Principal": "",\n "Action": "s3:PutObject",\n "Resource": "arn:aws:s3:::DOC-EXAMPLE-BUCKET/",\n "Condition": {\n "StringNotEquals": {\n "s3:x-amz-server-side-encryption": "AES256"\n }\n }\n },\n {\n "Sid": "DenyUnencryptedObjectUploads",\n "Effect": "Deny",\n "Principal": "",\n "Action": "s3:PutObject",\n "Resource": "arn:aws:s3:::DOC-EXAMPLE-BUCKET/",\n "Condition": {\n "Null": {\n "s3:x-amz-server-side-encryption": "true"\n }\n }\n }\n ]\n}\n\
        ```\n\n\
        Incorrect options:  Invoke the PutObject API operation and set the x-amz-server-side-encryption header as aws:kms. Use an S3 bucket policy to deny permission to upload an object unless the request has this header - As mentioned above, you need to use AES256 rather than aws:kms for the given use case. aws:kms is used when you want to use server-side encryption with AWS KMS (SSE-KMS).  Invoke the PutObject API operation and set the x-amz-server-side-encryption header as sse:s3. Use an S3 bucket policy to deny permission to upload an object unless the request has this header - This is a made-up option as the x-amz-server-side-encryption header has no such value as sse:s3.  Set the encryption key for SSE-S3 in the HTTP header of every request. Use an S3 bucket policy to deny permission to upload an object unless the request has this header - This option has been added as a distractor. For SSE-S3, Amazon S3 encrypts each object with a unique key. As an additional safeguard, it encrypts the key itself with a key that it rotates regularly. The encryption key for SSE-S3 encryption key cannot be accessed.'
    },
    {
        prompt: "You have created an Elastic Load Balancer that has marked all the EC2 instances in the target group as unhealthy. Surprisingly, when you enter the IP address of the EC2 instances in your web browser, you can access your website.  What could be the reason your instances are being marked as unhealthy? (Select two)",
        example: "The security group of the EC2 instance does not allow for traffic from the security group of the Application Load Balancer  The route for the health check is misconfigured",
        description: "You must ensure that your load balancer can communicate with registered targets on both the listener port and the health check port. Whenever you add a listener to your load balancer or update the health check port for a target group used by the load balancer to route requests, you must verify that the security groups associated with the load balancer allow traffic on the new port in both directions.",
        term: "security group nad health check"
    },
    {
        prompt: "After a test deployment in ElasticBeanstalk environment, a developer noticed that all accumulated Amazon EC2 burst balances were lost. Which of the following options can lead to this behavior?  a) The deployment was either run with immutable updates or in traffic splitting mode b) The deployment was run as a Rolling deployment, resulting in the resetting of EC2 burst balances c) When a canary deployment fails, it resets the EC2 burst balances to 0 d) The deployment was run as a All-at-once deployment, flushing all the accumulated EC2 burst balances",
        example: "The deployment was either run with immutable updates or in traffic splitting mode",
        description: "EC2 burst balance is a feature of Amazon Elastic Compute Cloud (EC2) instances that allows instances to temporarily use additional CPU credits beyond their baseline performance level.\n\
        - Immutable deployments perform an immutable update to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version. Immutable deployments can prevent issues caused by partially completed rolling deployments.  Traffic-splitting deployments let you perform canary testing as part of your application deployment. In a traffic-splitting deployment, Elastic Beanstalk launches a full set of new instances just like during an immutable deployment. It then forwards a specified percentage of incoming client traffic to the new application version for a specified evaluation period.  Some policies replace all instances during the deployment or update. This causes all accumulated Amazon EC2 burst balances to be lost. It happens in the following cases:      Managed platform updates with instance replacement enabled     Immutable updates     Deployments with immutable updates or traffic splitting enabled  Incorrect options:  The deployment was run as a Rolling deployment, resulting in the resetting of EC2 burst balances - With rolling deployments, Elastic Beanstalk splits the environment's Amazon EC2 instances into batches and deploys the new version of the application to one batch at a time. Rolling deployments do not result in loss of EC2 burst balances.  The deployment was run as a All-at-once deployment, flushing all the accumulated EC2 burst balances - The traditional All-at-once deployment, wherein all the instances are updated simultaneously, does not result in loss of EC2 burst balances.  When a canary deployment fails, it resets the EC2 burst balances to zero - This is incorrect and given only as a distractor.",
        term: "burst lost"
    },
    // TODO Create the terms here:
    {
        prompt: "Your company has stored all application secrets in SSM Parameter Store. The audit team has requested to get a report to better understand when and who has issued API calls against SSM Parameter Store. Which of the following options can be used to produce your report?  - Use SSM Parameter Store Access Logs in S3 to get a record of actions taken by a user - Use SSM Parameter Store Access Logs in CloudWatch Logs to get a record of actions taken by a user - Use AWS CloudTrail to get a record of actions taken by a user - Use SSM Parameter Store List feature to get a record of actions taken by a user",
        example: "Use AWS CloudTrail to get a record of actions taken by a user  AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data.  AWS CloudTrail provides a record of actions taken by a user, role, or an AWS service in Systems Manager. Using the information collected by AWS CloudTrail, you can determine the request that was made to Systems Manager, the IP address from which the request was made, who made the request, when it was made, and additional details.",
        description: "INCORRECT: Use SSM Parameter Store List feature to get a record of actions taken by a user - This option has been added as a distractor.  Use SSM Parameter Store Access Logs in CloudWatch Logs to get a record of actions taken by a user - CloudWatch Logs can be integrated but that will not help determine who issued API calls.  Use SSM Parameter Store Access Logs in S3 to get a record of actions taken by a user - S3 Access Logs can be integrated but that will not help determine who issued API calls.",
        
    },
    {
        prompt: "A developer has been asked to create a web application to be deployed on EC2 instances. The developer just wants to focus on writing application code without worrying about server provisioning, configuration and deployment.  As a Developer Associate, which AWS service would you recommend for the given use-case?",
        example: "Elastic Beanstalk  AWS Elastic Beanstalk provides an environment to easily deploy and run applications in the cloud. It is integrated with developer tools and provides a one-stop experience for you to manage the lifecycle of your applications.  AWS Elastic Beanstalk lets you manage all of the resources that run your application as environments where each environment runs only a single application version at a time. When an environment is being created, Elastic Beanstalk provisions all the required resources needed to run the application version. You don't need to worry about server provisioning, configuration, and deployment as that's taken care of by Beanstalk.",
        description: "INCORRECT: CloudFormation - AWS CloudFormation is a service that gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion. With CloudFormation, you still need to create a template to specify the type of resources you need, hence this option is not correct.  How CloudFormation Works: via - https://aws.amazon.com/cloudformation/  CodeDeploy - AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications. It can deploy an application to an instance but it cannot provision the instance.  Serverless Application Model - The AWS Serverless Application Model (AWS SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. You define the application you want with just a few lines per resource and model it using YAML. As the web application needs to be deployed on EC2 instances, so this option is ruled out."
    },
    {
        prompt: "The development team at a company creates serverless solutions using AWS Lambda. Functions are invoked by clients via AWS API Gateway which anyone can access. The team lead would like to control access using a 3rd party authorization mechanism.  As a Developer Associate, which of the following options would you recommend for the given use-case?",
        example: "Lambda Authorizer\n\n An Amazon API Gateway Lambda authorizer (formerly known as a custom authorizer) is a Lambda function that you provide to control access to your API. A Lambda authorizer uses bearer token authentication strategies, such as OAuth or SAML. Before creating an API Gateway Lambda authorizer, you must first create the AWS Lambda function that implements the logic to authorize and, if necessary, to authenticate the caller.",
        attachment: "./img/2023-04-21-14-43-06.png",
        description: `INCORRECT: "IAM permissions with sigv4" - Signature Version 4 is the process to add authentication information to AWS requests sent by HTTP. You will still need to provide permissions but our requirements have a need for 3rd party authentication which is where Lambda Authorizer comes in to play.  "Cognito User Pools" - A Cognito user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK. This is managed by AWS, therefore, does not meet our requirements.  "API Gateway User Pools" - This is a made-up option, added as a distractor.`
    },
    {
        prompt: "The development team has just configured and attached the IAM policy needed to access AWS Billing and Cost Management for all users under the Finance department. But, the users are unable to see AWS Billing and Cost Management service in the AWS console.  What could be the reason for this issue?",
        example: "You need to activate IAM user access to the Billing and Cost Management console for all the users who need access - By default, IAM users do not have access to the AWS Billing and Cost Management console. You or your account administrator must grant users access. You can do this by activating IAM user access to the Billing and Cost Management console and attaching an IAM policy to your users. Then, you need to activate IAM user access for IAM policies to take effect. You only need to activate IAM user access once.",
        description: "INCORRECT: The users might have another policy that restricts them from accessing the Billing information - This is an incorrect option, as deduced from the given use-case.  Only root user has access to AWS Billing and Cost Management console - This is an incorrect statement. AWS Billing and Cost Management access can be provided to any user through user activation and policies, as discussed above.  IAM user should be created under AWS Billing and Cost Management and not under the AWS account to have access to Billing console - IAM is a feature of your AWS account. All IAM users are created and managed from a single place, irrespective of the services they wish to you."
    },
    {
        prompt: "A development team lead is responsible for managing access for her IAM principals. At the start of the cycle, she has granted excess privileges to users to keep them motivated for trying new things. She now wants to ensure that the team has only the minimum permissions required to finish their work.  Which of the following will help her identify unused IAM roles and remove them without disrupting any service?",
        example: "Access Advisor feature on IAM console- To help identify the unused roles, IAM reports the last-used timestamp that represents when a role was last used to make an AWS request. Your security team can use this information to identify, analyze, and then confidently remove unused roles. This helps improve the security posture of your AWS environments. Additionally, by removing unused roles, you can simplify your monitoring and auditing efforts by focusing only on roles that are in use.",
        description: "Incorrect options:  AWS Trusted Advisor - AWS Trusted Advisor is an online tool that provides you real-time guidance to help you provision your resources following AWS best practices on cost optimization, security, fault tolerance, service limits, and performance improvement.  IAM Access Analyzer - AWS IAM Access Analyzer helps you identify the resources in your organization and accounts, such as Amazon S3 buckets or IAM roles, that are shared with an external entity. This lets you identify unintended access to your resources and data, which is a security risk.  Amazon Inspector - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices."
    },
    {
        prompt: "You're a developer working on a large scale order processing application. After developing the features, you commit your code to AWS CodeCommit and begin building the project with AWS CodeBuild before it gets deployed to the server. The build is taking too long and the error points to an issue resolving dependencies from a third-party. You would like to prevent a build running this long in the future for similar underlying reasons.  Which of the following options represents the best solution to address this use-case?",
        example: "Enable CodeBuild timeouts     A build represents a set of actions performed by AWS CodeBuild to create output artifacts (for example, a JAR file) based on a set of input artifacts (for example, a collection of Java class files).  The following rules apply when you run multiple builds:  When possible, builds run concurrently. The maximum number of concurrently running builds can vary.  Builds are queued if the number of concurrently running builds reaches its limit. The maximum number of builds in a queue is five times the concurrent build limit.  A build in a queue that does not start after the number of minutes specified in its time out value is removed from the queue. The default timeout value is eight hours. You can override the build queue timeout with a value between five minutes and eight hours when you run your build.  By setting the timeout configuration, the build process will automatically terminate post the expiry of the configured timeout.",
        description: "Incorrect options:  Use AWS Lambda - AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Lambda cannot be used to impact the code build process.  Use AWS CloudWatch Events - Amazon CloudWatch allows you to monitor AWS cloud resources and the applications you run on AWS. Metrics are provided automatically for a number of AWS products and services. CloudWatch is good for monitoring and viewing logs. CloudWatch cannot be used to impact the code build process.  Use VPC Flow Logs - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC but not for code compiling configuration. VPC Flow Logs cannot be used to impact the code build process."
    },
    {
        prompt: "As an AWS Certified Developer Associate, you are given a document written in YAML that represents the architecture of a serverless application. The first line of the document contains Transform: 'AWS::Serverless-2016-10-31'.  What does the Transform section in the document represent?",
        example: `AWS CloudFormation template is a JSON- or YAML-formatted text file that describes your AWS infrastructure. Templates include several major sections. The "Resources" section is the only required section. The optional "Transform" section specifies one or more macros that AWS CloudFormation uses to process your template.  The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML.  Presence of 'Transform' section indicates it is a Serverless Application Model (SAM) template - The AWS::Serverless transform, which is a macro hosted by AWS CloudFormation, takes an entire template written in the AWS Serverless Application Model (AWS SAM) syntax and transforms and expands it into a compliant AWS CloudFormation template. So, presence of "Transform" section indicates, the document is a SAM template.`,
        attachment: "./img/2023-04-21-14-57-59.png",
        description: `It represents a Lambda function definition - Lambda function is created using "AWS::Lambda::Function" resource and has no connection to 'Transform' section.  It represents an intrinsic function - Intrinsic Functions in templates are used to assign values to properties that are not available until runtime. They usually start with Fn:: or !. Example: !Sub or Fn::Sub.  Presence of 'Transform' section indicates it is a CloudFormation Parameter - CloudFormation parameters are part of Parameters block of the template, like so:  Parameters in YAML: `
    },
    {
        prompt: "An organization has offices across multiple locations and the technology team has configured an Application Load Balancer across targets in multiple Availability Zones. The team wants to analyze the incoming requests for latencies and the client's IP address patterns.  Which feature of the Load Balancer will help collect the required information?",
        example: "ALB access logs - Elastic Load Balancing provides access logs that capture detailed information about requests sent to your load balancer. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues. Access logging is an optional feature of Elastic Load Balancing that is disabled by default.",
        attachment: "./img/2023-04-21-15-02-12.png",
        description: "Incorrect options:  CloudTrail logs - Elastic Load Balancing is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in Elastic Load Balancing. CloudTrail captures all API calls for Elastic Load Balancing as events. You can use AWS CloudTrail to capture detailed information about the calls made to the Elastic Load Balancing API and store them as log files in Amazon S3. You can use these CloudTrail logs to determine which API calls were made, the source IP address where the API call came from, who made the call, when the call was made, and so on.  CloudWatch metrics - Elastic Load Balancing publishes data points to Amazon CloudWatch for your load balancers and your targets. CloudWatch enables you to retrieve statistics about those data points as an ordered set of time-series data, known as metrics. You can use metrics to verify that your system is performing as expected. This is the right feature if you wish to track a certain metric.  ALB request tracing - You can use request tracing to track HTTP requests. The load balancer adds a header with a trace identifier to each request it receives. Request tracing will not help you to analyze latency specific data."
    },
    {
        prompt: "Which of the following security credentials can only be created by the AWS Account root user? ",
        example: "CloudFront Key Pairs - IAM users can't create CloudFront key pairs. You must log in using root credentials to create key pairs.  To create signed URLs or signed cookies, you need a signer. A signer is either a trusted key group that you create in CloudFront, or an AWS account that contains a CloudFront key pair. AWS recommends that you use trusted key groups with signed URLs and signed cookies instead of using CloudFront key pairs.",
        description: "Incorrect options:  EC2 Instance Key Pairs - You use key pairs to access Amazon EC2 instances, such as when you use SSH to log in to a Linux instance. These key pairs can be created from the IAM user login and do not need root user access.  IAM User Access Keys - Access keys consist of two parts: an access key ID and a secret access key. You use access keys to sign programmatic requests that you make to AWS if you use AWS CLI commands (using the SDKs) or using AWS API operations. IAM users can create their own Access Keys, does not need root access.  IAM User passwords - Every IAM user has access to his own credentials and can reset the password whenever they need to."
    },
    {
        prompt: "A gaming company wants to store information about all the games that the company has released. Each game has a name, version number, and category (such as sports, puzzles, strategy, etc). The game information also can include additional properties about the supported platforms and technical specifications. This additional information is inconsistent across games.  You have been hired as an AWS Certified Developer Associate to build a solution that addresses the following use cases:  For a given name and version number, get all details about the game that has that name and version number.  For a given name, get all details about all games that have that name.  For a given category, get all details about all games in that category.  What will you recommend as the most efficient solution?",
        example: "Set up an Amazon DynamoDB table with a primary key that consists of the name as the partition key and the version number as the sort key. Create a global secondary index that has the category as the partition key and the name as the sort key",
        description: "When you create a DynamoDB table, in addition to the table name, you must specify the primary key of the table. The primary key uniquely identifies each item in the table, so that no two items can have the same key. You can create one or more secondary indexes on a table. A secondary index lets you query the data in the table using an alternate key, in addition to queries against the primary key. DynamoDB doesn't require that you use indexes, but they give your applications more flexibility when querying your data.\n\
        \n\n\
        Incorrect options:  Set up an Amazon DynamoDB table with a primary key that consists of the category as the partition key and the version number as the sort key. Create a global secondary index that has the name as the partition key - The DynamoDB table for this option has the primary key and GSI that do not solve for the condition - \"For a given name and version number, get all details about the game that has that name and version number\". This option does not allow for efficient querying of a specific game by its name and version number as you need multiple queries which would be less efficient than the single query allowed by the correct option.  Set up an Amazon RDS MySQL instance having a games table that contains columns for name, version number, and category. Configure the name column as the primary key - This option is not the right fit as it does not allow you to efficiently query on the version number and category columns.  Permanently store the name, version number, and category information about the games in an Amazon Elasticache for Memcached instance - You cannot use Elasticache for Memcached to permanently store values meant to be persisted in a database (relational or NoSQL). Elasticache is a caching layer. So this option is incorrect.",
        attachment: "./img/2023-04-21-15-10-37.png"
    },
    {
        prompt: "A retail company is migrating its on-premises database to Amazon RDS for PostgreSQL. The company has read-heavy workloads. The development team at the company is looking at refactoring the code to achieve optimum read performance for SQL queries.  Which solution will address this requirement with the least current as well as future development effort?",
        example: "Set up Amazon RDS with one or more read replicas. Refactor the application code so that the queries use the endpoint for the read replicas\n\
        Amazon RDS uses the PostgreSQL DB engine's built-in replication functionality to create a special type of DB instance called a read replica from a source DB instance. The source DB instance becomes the primary DB instance. Updates made to the primary DB instance are asynchronously copied to the read replica. You can reduce the load on your primary DB instance by routing read queries from your applications to the read replica. Using read replicas, you can elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the given use case, you can achieve optimum read performance for SQL queries by using the read-replica endpoint for the read-heavy workload.",
        attachment: "./img/2023-04-21-15-17-48.png",
        description: "Incorrect options:  Configure Elasticache for Redis to act as a caching layer for Amazon RDS. Refactor the application code so that the queries use the Elasticache for Redis endpoint  Configure Elasticache for Memcached to act as a caching layer for Amazon RDS. Refactor the application code so that the queries use the Elasticache for Memcached endpoint  Both Redis and Memcached are popular, open-source, in-memory data stores (also known as in-memory caches). These are not relational databases and cannot be used to run SQL queries. So, both these options are incorrect.  Set up Amazon RDS in the multi-AZ configuration with a single standby instance. Refactor the application code so that the queries use the standby instance endpoint - In an Amazon RDS Multi-AZ deployment with a single standby instance, Amazon RDS automatically creates a primary database (DB) instance and synchronously replicates the data to an instance in a different AZ. When it detects a failure, Amazon RDS automatically fails over to a standby instance without manual intervention. You cannot route the read queries from an application to the standby instance of a multi-AZ RDS database as it's not accessible for the read traffic in the single standby instance configuration."
    },
    {
        prompt: "A developer is configuring a bucket policy that denies upload object permission to any requests that do not include the x-amz-server-side-encryption header requesting server-side encryption with SSE-KMS for an Amazon S3 bucket - examplebucket.  Which of the following policies is the right fit for the given requirement?",
        description:`{ "StringNotEquals":{ "s3:x-amz-server-side-encryption":"aws:kms" } } } ] }\n\n\
        { "Version":"2012-10-17", "Id":"PutObjectPolicy", "Statement":[{ "Sid":"DenyUnEncryptedObjectUploads", "Effect":"Deny", "Principal":"", "Action":"s3:PutObject", "Resource":"arn:aws:s3:::examplebucket/", "Condition": .... `,
        example:`{ "Version":"2012-10-17", "Id":"PutObjectPolicy", "Statement":[{ "Sid":"DenyUnEncryptedObjectUploads", "Effect":"Deny", "Principal":"", "Action":"s3:PutObject", "Resource":"arn:aws:s3:::examplebucket/", "Condition":{ "StringNotEquals":{ "s3:x-amz-server-side-encryption":"aws:kms" } } } ] } `,
    },
    {
        prompt: "As an AWS Certified Developer Associate, you have configured the AWS CLI on your workstation. Your default region is us-east-1 and your IAM user has permissions to operate commands on services such as EC2, S3 and RDS in any region. You would like to execute a command to stop an EC2 instance in the us-east-2 region.  What of the following is the MOST optimal solution to address this use-case?",
        example: "Use the --region parameter: If the region parameter is not set, then the CLI command is executed against the default AWS region.  You can also review all general options for AWS CLI: ",
        attachment: "./img/2023-04-21-15-25-31.png",
        description: "Incorrect options:  You need to override the default region by using aws configure - This is not the most optimal way as you will have to change it again to reset the default region.  You should create a new IAM user just for that other region - This is not the most optimal way as you would need to manage two IAM user profiles.  Use boto3 dependency injection - With the CLI you do not use boto3. This option is a distractor."
    },
    {
        prompt: "A company uses Elastic Beanstalk to manage its IT infrastructure on AWS Cloud and it would like to deploy the new application version to the EC2 instances. When the deployment is executed, some instances should serve requests with the old application version, while other instances should serve requests using the new application version until the deployment is completed.  Which deployment meets this requirement without incurring additional costs?",
        example: "With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.  How Elastic BeanStalk Works: via - https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html  The rolling deployment policy deploys the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment's capacity by the number of instances in a batch. The cost remains the same as the number of EC2 instances does not increase. This policy avoids downtime and minimizes reduced availability, at a cost of a longer deployment time.",
        attachment: "./img/2023-04-21-15-37-18.png",
        description: "Incorrect options:  Immutable - The 'Immutable' deployment policy ensures that your new application version is always deployed to new instances, instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails.  All at once - This policy deploys the new version to all instances simultaneously. Although 'All at once' is the quickest deployment method, but the application may become unavailable to users (or have low availability) for a short time.  Rolling with additional batches - This policy deploys the new version in batches, but first launches a new batch of instances to ensure full capacity during the deployment process. This policy avoids any reduced availability, at a cost of an even longer deployment time compared to the Rolling method. Suitable if you must maintain the same bandwidth throughout the deployment. These increase the costs as you're adding extra instances during the deployment.  Reference:  https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html"
    },
    {
        prompt: "An IT company is configuring Auto Scaling for its Amazon EC2 instances spread across different AZs and Regions.  Which of the following scenarios are NOT correct about EC2 Auto Scaling? (Select two)\n\n\
        options:\n\
        ",
        example: "An IT company is configuring Auto Scaling for its Amazon EC2 instances spread across different AZs and Regions. Which of the following scenarios are NOT correct about EC2 Auto Scaling? ( Select two)  - Auto Saling groups that span across multiple Regions need to be enabled for all the Regions specified - Amazon EC2 Auto Scaling attempts to distribute instances evenly between the AZs that are enabled for your Auto Scaling group - An Auto Scaling group can contain EC2 instances in one or more AZs within the same Region - ForAuto Scaling groups in a VPC, the EC2 instances are launched in subnets - An Auto Scaling group can contain instances in only on AZ zone of a region",
        attachment: "./img/2023-04-21-15-46-04.png",
        description: "Incorrect options:  An Auto Scaling group can contain EC2 instances in one or more Availability Zones within the same Region - This is a valid statement. Auto Scaling groups can span across the availability Zones of a Region.  Amazon EC2 Auto Scaling attempts to distribute instances evenly between the Availability Zones that are enabled for your Auto Scaling group - When one Availability Zone becomes unhealthy or unavailable, Auto Scaling launches new instances in an unaffected Availability Zone. When the unhealthy Availability Zone returns to a healthy state, Auto Scaling automatically redistributes the application instances evenly across all of the designated Availability Zones.  For Auto Scaling groups in a VPC, the EC2 instances are launched in subnets - For Auto Scaling groups in a VPC, the EC2 instances are launched in subnets. Customers can select the subnets for your EC2 instances when you create or update the Auto Scaling group."
    },
    {
        prompt: "A development team lead is configuring policies for his team at an IT company. Which of the following policy types only limit permissions but cannot grant permissions (Select two)?  - AWS Organizations Service Control Policy (SCP) - Resource-based policy - Permissions boundary - Identity-based policy - Access control list (ACL)",
        example: "Correct options:  AWS Organizations Service Control Policy (SCP) – Use an AWS Organizations Service Control Policy (SCP) to define the maximum permissions for account members of an organization or organizational unit (OU). SCPs limit permissions that identity-based policies or resource-based policies grant to entities (users or roles) within the account, but do not grant permissions.  Permissions boundary - Permissions boundary is a managed policy that is used for an IAM entity (user or role). The policy defines the maximum permissions that the identity-based policies can grant to an entity, but does not grant permissions.",
        attachment: "./img/2023-04-21-15-51-47.png",
        description: "INCORRECT: Access control list (ACL) - Use ACLs to control which principals in other accounts can access the resource to which the ACL is attached. ACLs are similar to resource-based policies, although they are the only policy type that does not use the JSON policy document structure. ACLs are cross-account permissions policies that grant permissions to the specified principal.  Resource-based policy - Resource-based policies grant permissions to the principal that is specified in the policy. Principals can be in the same account as the resource or in other accounts. The most common examples of resource-based policies are Amazon S3 bucket policies and IAM role trust policies.  Identity-based policy - Help attach managed and inline policies to IAM identities (users, groups to which users belong, or roles). Identity-based policies grant permissions to an identity."
    },
    {
        prompt: "You have chosen AWS Elastic Beanstalk to upload your application code and allow it to handle details such as provisioning resources and monitoring.  When creating configuration files for AWS Elastic Beanstalk which naming convention should you follow?\n\
        for config | <mysettings> and ebexetensions",
        example: ".ebextensions/<mysettings>.config : You can add AWS Elastic Beanstalk configuration files (.ebextensions) to your web application's source code to configure your environment and customize the AWS resources that it contains. Configuration files are YAML or JSON formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle.",
        attachment: "./img/2023-04-21-15-56-45.png"
    },
    {
        prompt: "You are a developer for a web application written in .NET which uses the AWS SDK. You need to implement an authentication mechanism that returns a JWT (JSON Web Token).  Which AWS service will help you with token handling and management?",
        example: `Correct option:  "Cognito User Pools"  After successful authentication, Amazon Cognito returns user pool tokens to your app. You can use the tokens to grant your users access to your own server-side resources, or to the Amazon API Gateway.  Amazon Cognito user pools implement ID, access, and refresh tokens as defined by the OpenID Connect (OIDC) open standard.  The ID token is a JSON Web Token (JWT) that contains claims about the identity of the authenticated user such as name, email, and phone_number. You can use this identity information inside your application. The ID token can also be used to authenticate users against your resource servers or server applications.`,
        attachment: "./img/2023-04-21-16-00-28.png"
    }


]


module.exports = {
    aws_certification_associate_developer
}