
const aws_certification_associate_developer = [
    {
        prompt: "A startup with newly created AWS account is testing different EC2 instances. They have used Burstable performance instance - T2.micro - for 35 seconds and stopped the instance.  At the end of the month, what is the instance usage duration that the company is charged for?",
        example: "0 seconds",
        term: "T2.micro",
    },
    {
        prompt: "An application is hosted by a 3rd party and exposed at yourapp.3rdparty.com. You would like to have your users access your application using www.mydomain.com, which you own and manage under Route 53.  What Route 53 record should you create?",
        example: "A CNAME record\n\n\
        A CNAME record maps DNS queries for the name of the current record, such as acme.example.com, to another domain (example.com or example.net) or subdomain (acme.example.com or zenith.example.org).  CNAME records can be used to map one domain name to another. Although you should keep in mind that the DNS protocol does not allow you to create a CNAME record for the top node of a DNS namespace, also known as the zone apex. For example, if you register the DNS name example.com, the zone apex is example.com. You cannot create a CNAME record for example.com, but you can create CNAME records for www.example.com, newproduct.example.com, and so on.",
        term: "domain name system records", 
        description: "A Record (Address Record):  An A record is a type of DNS (Domain Name System) record that maps a domain name to an IPv4 address. In other words, an A record is used to translate a human-readable domain name (such as www.example.com) into a machine-readable IP address (such as 192.0.2.1).  Alias Record (ANAME or ALIAS):  Alias records are DNS records that allow a DNS query for a domain name to be redirected to another domain name. An alias record is similar to a CNAME record, but it allows the root domain to be used in the DNS query. Alias records are useful for pointing a domain name to a service that is hosted on another domain name, such as pointing a subdomain to a load balancer.  CNAME (Canonical Name) Record:  A CNAME record is a type of DNS record that maps one domain name to another. This is useful for creating aliases for a domain or for pointing a subdomain to another domain. For example, if you have a subdomain called shop.example.com and you want it to point to another domain called store.example.net, you can create a CNAME record that maps shop.example.com to store.example.net.  PTR (Pointer) Record:  A PTR record is a type of DNS record that maps an IP address to a domain name. PTR records are used in reverse DNS (rDNS) lookups to determine the domain name associated with a given IP address. This is useful for verifying the identity of a server or for troubleshooting network issues."
    },
    {
        prompt: "A developer has been asked to create an application that can be deployed across a fleet of EC2 instances. The configuration must allow for full control over the deployment steps using the blue-green deployment.  Which service will help you achieve that?",
        example: "AWS CodeDeploy\n\n\
        AWS CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, or serverless Lambda functions. AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy.  The blue/green deployment type uses the blue/green deployment model controlled by CodeDeploy. This deployment type enables you to verify a new deployment of service before sending production traffic to it.",
        term: "AWS blue  green deployment",
        description: `Blue-green deployment is a deployment strategy that involves creating two identical environments, one "blue" and one "green," where only one of the environments is live at a time.  In this deployment model, the current production environment, or "blue" environment, remains active and serving traffic while a new "green" environment is deployed with the latest code changes and tested to ensure it is functioning properly. Once the "green" environment has been verified and is ready to go live, traffic is routed to it while the "blue" environment is taken down and updated with any necessary changes.  The process can be repeated in the future, with the "blue" and "green" environments swapping roles. This deployment approach reduces the risk of downtime or errors during the deployment process by enabling the new version to be fully tested before it goes live, and allowing for quick rollbacks if issues arise. It also provides a way to achieve zero-downtime deployments, as the switch from "blue" to "green" can be done seamlessly without impacting end-users.\n\
        AWS CodeDeploy is designed to complement other AWS deployment services like AWS CodePipeline, AWS CodeBuild, and AWS Elastic Beanstalk, and offers unique advantages in certain scenarios.  While AWS CodePipeline and AWS CodeBuild can be used to automate the build, test, and deployment process, they don't offer the same level of control and customization over the deployment process as AWS CodeDeploy. AWS CodeDeploy provides more granular control over the deployment process and can handle more complex deployment scenarios, such as blue-green deployments and canary releases.  AWS Elastic Beanstalk, on the other hand, is a fully managed platform that automatically handles the deployment and management of applications, but it provides less control over the deployment process and requires the use of specific platforms and configurations.  Overall, AWS CodeDeploy offers advantages in scenarios where a higher degree of control and customization over the deployment process is required, and where more complex deployment scenarios are needed, such as when deploying to on-premises servers, Lambda functions, or other compute services that are not supported by AWS Elastic Beanstalk.`
    },
    {
        prompt: "You are a developer working on AWS Lambda functions that are invoked via REST API's using Amazon API Gateway. Currently, when a GET request is invoked by the consumer, the entire data-set returned by the Lambda function is visible. Your team lead asked you to format the data response.  Which feature of the API Gateway can be used to solve this issue?",
        example: "Mapping Templates\n\n\
        Use API Gateway Mapping Templates - In API Gateway, an API's method request can take a payload in a different format from the corresponding integration request payload, as required in the backend. Similarly, vice versa is also possible. API Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response.",
        term: "mapping reponses"
    },
    {
        prompt: "A developer has an application that stores data in an Amazon S3 bucket. The application uses an HTTP API to store and retrieve objects. When the PutObject API operation adds objects to the S3 bucket the developer must encrypt these objects at rest by using server-side encryption with Amazon S3-managed keys (SSE-S3).  Which solution will guarantee that any upload request without the mandated encryption is not processed?",
        example: "Invoke the PutObject API operation and set the x-amz-server-side-encryption header as AES256. Use an S3 bucket policy to deny permission to upload an object unless the request has this header",
        description: ':m SSE-S3 server-side encryption protects data at rest. Amazon S3 encrypts each object with a unique key. As an additional safeguard, it encrypts the key itself with a key that it rotates regularly. Amazon S3 server-side encryption uses one of the strongest block ciphers available to encrypt your data, 256-bit Advanced Encryption Standard (AES-256).  You can use the following bucket policy to deny permissions to upload an object unless the request includes the x-amz-server-side-encryption header to request server-side encryption using SSE-S3:\n\n\
        ```json\n\
        {\n "Version": "2012-10-17",\n "Id": "PutObjectPolicy",\n "Statement": [\n {\n "Sid": "DenyIncorrectEncryptionHeader",\n "Effect": "Deny",\n "Principal": "",\n "Action": "s3:PutObject",\n "Resource": "arn:aws:s3:::DOC-EXAMPLE-BUCKET/",\n "Condition": {\n "StringNotEquals": {\n "s3:x-amz-server-side-encryption": "AES256"\n }\n }\n },\n {\n "Sid": "DenyUnencryptedObjectUploads",\n "Effect": "Deny",\n "Principal": "",\n "Action": "s3:PutObject",\n "Resource": "arn:aws:s3:::DOC-EXAMPLE-BUCKET/",\n "Condition": {\n "Null": {\n "s3:x-amz-server-side-encryption": "true"\n }\n }\n }\n ]\n}\n\
        ```\n\n\
        Incorrect options:  Invoke the PutObject API operation and set the x-amz-server-side-encryption header as aws:kms. Use an S3 bucket policy to deny permission to upload an object unless the request has this header - As mentioned above, you need to use AES256 rather than aws:kms for the given use case. aws:kms is used when you want to use server-side encryption with AWS KMS (SSE-KMS).  Invoke the PutObject API operation and set the x-amz-server-side-encryption header as sse:s3. Use an S3 bucket policy to deny permission to upload an object unless the request has this header - This is a made-up option as the x-amz-server-side-encryption header has no such value as sse:s3.  Set the encryption key for SSE-S3 in the HTTP header of every request. Use an S3 bucket policy to deny permission to upload an object unless the request has this header - This option has been added as a distractor. For SSE-S3, Amazon S3 encrypts each object with a unique key. As an additional safeguard, it encrypts the key itself with a key that it rotates regularly. The encryption key for SSE-S3 encryption key cannot be accessed.'
    },
    {
        prompt: "You have created an Elastic Load Balancer that has marked all the EC2 instances in the target group as unhealthy. Surprisingly, when you enter the IP address of the EC2 instances in your web browser, you can access your website.  What could be the reason your instances are being marked as unhealthy? (Select two)",
        example: "The security group of the EC2 instance does not allow for traffic from the security group of the Application Load Balancer  The route for the health check is misconfigured",
        description: "You must ensure that your load balancer can communicate with registered targets on both the listener port and the health check port. Whenever you add a listener to your load balancer or update the health check port for a target group used by the load balancer to route requests, you must verify that the security groups associated with the load balancer allow traffic on the new port in both directions.",
        term: "security group nad health check"
    },
    {
        prompt: "After a test deployment in ElasticBeanstalk environment, a developer noticed that all accumulated Amazon EC2 burst balances were lost. Which of the following options can lead to this behavior?  a) The deployment was either run with immutable updates or in traffic splitting mode b) The deployment was run as a Rolling deployment, resulting in the resetting of EC2 burst balances c) When a canary deployment fails, it resets the EC2 burst balances to 0 d) The deployment was run as a All-at-once deployment, flushing all the accumulated EC2 burst balances",
        example: "The deployment was either run with immutable updates or in traffic splitting mode",
        description: "EC2 burst balance is a feature of Amazon Elastic Compute Cloud (EC2) instances that allows instances to temporarily use additional CPU credits beyond their baseline performance level.\n\
        - Immutable deployments perform an immutable update to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version. Immutable deployments can prevent issues caused by partially completed rolling deployments.  Traffic-splitting deployments let you perform canary testing as part of your application deployment. In a traffic-splitting deployment, Elastic Beanstalk launches a full set of new instances just like during an immutable deployment. It then forwards a specified percentage of incoming client traffic to the new application version for a specified evaluation period.  Some policies replace all instances during the deployment or update. This causes all accumulated Amazon EC2 burst balances to be lost. It happens in the following cases:      Managed platform updates with instance replacement enabled     Immutable updates     Deployments with immutable updates or traffic splitting enabled  Incorrect options:  The deployment was run as a Rolling deployment, resulting in the resetting of EC2 burst balances - With rolling deployments, Elastic Beanstalk splits the environment's Amazon EC2 instances into batches and deploys the new version of the application to one batch at a time. Rolling deployments do not result in loss of EC2 burst balances.  The deployment was run as a All-at-once deployment, flushing all the accumulated EC2 burst balances - The traditional All-at-once deployment, wherein all the instances are updated simultaneously, does not result in loss of EC2 burst balances.  When a canary deployment fails, it resets the EC2 burst balances to zero - This is incorrect and given only as a distractor.",
        term: "burst lost"
    },
    // TODO Create the terms here:
    {
        prompt: "Your company has stored all application secrets in SSM Parameter Store. The audit team has requested to get a report to better understand when and who has issued API calls against SSM Parameter Store. Which of the following options can be used to produce your report?  - Use SSM Parameter Store Access Logs in S3 to get a record of actions taken by a user - Use SSM Parameter Store Access Logs in CloudWatch Logs to get a record of actions taken by a user - Use AWS CloudTrail to get a record of actions taken by a user - Use SSM Parameter Store List feature to get a record of actions taken by a user",
        example: "Use AWS CloudTrail to get a record of actions taken by a user  AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data.  AWS CloudTrail provides a record of actions taken by a user, role, or an AWS service in Systems Manager. Using the information collected by AWS CloudTrail, you can determine the request that was made to Systems Manager, the IP address from which the request was made, who made the request, when it was made, and additional details.",
        description: "INCORRECT: Use SSM Parameter Store List feature to get a record of actions taken by a user - This option has been added as a distractor.  Use SSM Parameter Store Access Logs in CloudWatch Logs to get a record of actions taken by a user - CloudWatch Logs can be integrated but that will not help determine who issued API calls.  Use SSM Parameter Store Access Logs in S3 to get a record of actions taken by a user - S3 Access Logs can be integrated but that will not help determine who issued API calls.",
        
    },
    {
        prompt: "A developer has been asked to create a web application to be deployed on EC2 instances. The developer just wants to focus on writing application code without worrying about server provisioning, configuration and deployment.  As a Developer Associate, which AWS service would you recommend for the given use-case?",
        example: "Elastic Beanstalk  AWS Elastic Beanstalk provides an environment to easily deploy and run applications in the cloud. It is integrated with developer tools and provides a one-stop experience for you to manage the lifecycle of your applications.  AWS Elastic Beanstalk lets you manage all of the resources that run your application as environments where each environment runs only a single application version at a time. When an environment is being created, Elastic Beanstalk provisions all the required resources needed to run the application version. You don't need to worry about server provisioning, configuration, and deployment as that's taken care of by Beanstalk.",
        description: "INCORRECT: CloudFormation - AWS CloudFormation is a service that gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable fashion. With CloudFormation, you still need to create a template to specify the type of resources you need, hence this option is not correct.  How CloudFormation Works: via - https://aws.amazon.com/cloudformation/  CodeDeploy - AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications. It can deploy an application to an instance but it cannot provision the instance.  Serverless Application Model - The AWS Serverless Application Model (AWS SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. You define the application you want with just a few lines per resource and model it using YAML. As the web application needs to be deployed on EC2 instances, so this option is ruled out."
    },
    {
        prompt: "The development team at a company creates serverless solutions using AWS Lambda. Functions are invoked by clients via AWS API Gateway which anyone can access. The team lead would like to control access using a 3rd party authorization mechanism.  As a Developer Associate, which of the following options would you recommend for the given use-case?",
        example: "Lambda Authorizer\n\n An Amazon API Gateway Lambda authorizer (formerly known as a custom authorizer) is a Lambda function that you provide to control access to your API. A Lambda authorizer uses bearer token authentication strategies, such as OAuth or SAML. Before creating an API Gateway Lambda authorizer, you must first create the AWS Lambda function that implements the logic to authorize and, if necessary, to authenticate the caller.",
        attachment: "./img/2023-04-21-14-43-06.png",
        description: `INCORRECT: "IAM permissions with sigv4" - Signature Version 4 is the process to add authentication information to AWS requests sent by HTTP. You will still need to provide permissions but our requirements have a need for 3rd party authentication which is where Lambda Authorizer comes in to play.  "Cognito User Pools" - A Cognito user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito, or federate through a third-party identity provider (IdP). Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through an SDK. This is managed by AWS, therefore, does not meet our requirements.  "API Gateway User Pools" - This is a made-up option, added as a distractor.`
    },
    {
        prompt: "The development team has just configured and attached the IAM policy needed to access AWS Billing and Cost Management for all users under the Finance department. But, the users are unable to see AWS Billing and Cost Management service in the AWS console.  What could be the reason for this issue?",
        example: "You need to activate IAM user access to the Billing and Cost Management console for all the users who need access - By default, IAM users do not have access to the AWS Billing and Cost Management console. You or your account administrator must grant users access. You can do this by activating IAM user access to the Billing and Cost Management console and attaching an IAM policy to your users. Then, you need to activate IAM user access for IAM policies to take effect. You only need to activate IAM user access once.",
        description: "INCORRECT: The users might have another policy that restricts them from accessing the Billing information - This is an incorrect option, as deduced from the given use-case.  Only root user has access to AWS Billing and Cost Management console - This is an incorrect statement. AWS Billing and Cost Management access can be provided to any user through user activation and policies, as discussed above.  IAM user should be created under AWS Billing and Cost Management and not under the AWS account to have access to Billing console - IAM is a feature of your AWS account. All IAM users are created and managed from a single place, irrespective of the services they wish to you."
    },
    {
        prompt: "A development team lead is responsible for managing access for her IAM principals. At the start of the cycle, she has granted excess privileges to users to keep them motivated for trying new things. She now wants to ensure that the team has only the minimum permissions required to finish their work.  Which of the following will help her identify unused IAM roles and remove them without disrupting any service?",
        example: "Access Advisor feature on IAM console- To help identify the unused roles, IAM reports the last-used timestamp that represents when a role was last used to make an AWS request. Your security team can use this information to identify, analyze, and then confidently remove unused roles. This helps improve the security posture of your AWS environments. Additionally, by removing unused roles, you can simplify your monitoring and auditing efforts by focusing only on roles that are in use.",
        description: "Incorrect options:  AWS Trusted Advisor - AWS Trusted Advisor is an online tool that provides you real-time guidance to help you provision your resources following AWS best practices on cost optimization, security, fault tolerance, service limits, and performance improvement.  IAM Access Analyzer - AWS IAM Access Analyzer helps you identify the resources in your organization and accounts, such as Amazon S3 buckets or IAM roles, that are shared with an external entity. This lets you identify unintended access to your resources and data, which is a security risk.  Amazon Inspector - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. Amazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices."
    },
    {
        prompt: "You're a developer working on a large scale order processing application. After developing the features, you commit your code to AWS CodeCommit and begin building the project with AWS CodeBuild before it gets deployed to the server. The build is taking too long and the error points to an issue resolving dependencies from a third-party. You would like to prevent a build running this long in the future for similar underlying reasons.  Which of the following options represents the best solution to address this use-case?",
        example: "Enable CodeBuild timeouts     A build represents a set of actions performed by AWS CodeBuild to create output artifacts (for example, a JAR file) based on a set of input artifacts (for example, a collection of Java class files).  The following rules apply when you run multiple builds:  When possible, builds run concurrently. The maximum number of concurrently running builds can vary.  Builds are queued if the number of concurrently running builds reaches its limit. The maximum number of builds in a queue is five times the concurrent build limit.  A build in a queue that does not start after the number of minutes specified in its time out value is removed from the queue. The default timeout value is eight hours. You can override the build queue timeout with a value between five minutes and eight hours when you run your build.  By setting the timeout configuration, the build process will automatically terminate post the expiry of the configured timeout.",
        description: "Incorrect options:  Use AWS Lambda - AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Lambda cannot be used to impact the code build process.  Use AWS CloudWatch Events - Amazon CloudWatch allows you to monitor AWS cloud resources and the applications you run on AWS. Metrics are provided automatically for a number of AWS products and services. CloudWatch is good for monitoring and viewing logs. CloudWatch cannot be used to impact the code build process.  Use VPC Flow Logs - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC but not for code compiling configuration. VPC Flow Logs cannot be used to impact the code build process."
    },
    {
        prompt: "As an AWS Certified Developer Associate, you are given a document written in YAML that represents the architecture of a serverless application. The first line of the document contains Transform: 'AWS::Serverless-2016-10-31'.  What does the Transform section in the document represent?",
        example: `AWS CloudFormation template is a JSON- or YAML-formatted text file that describes your AWS infrastructure. Templates include several major sections. The "Resources" section is the only required section. The optional "Transform" section specifies one or more macros that AWS CloudFormation uses to process your template.  The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML.  Presence of 'Transform' section indicates it is a Serverless Application Model (SAM) template - The AWS::Serverless transform, which is a macro hosted by AWS CloudFormation, takes an entire template written in the AWS Serverless Application Model (AWS SAM) syntax and transforms and expands it into a compliant AWS CloudFormation template. So, presence of "Transform" section indicates, the document is a SAM template.`,
        attachment: "./img/2023-04-21-14-57-59.png",
        description: `It represents a Lambda function definition - Lambda function is created using "AWS::Lambda::Function" resource and has no connection to 'Transform' section.  It represents an intrinsic function - Intrinsic Functions in templates are used to assign values to properties that are not available until runtime. They usually start with Fn:: or !. Example: !Sub or Fn::Sub.  Presence of 'Transform' section indicates it is a CloudFormation Parameter - CloudFormation parameters are part of Parameters block of the template, like so:  Parameters in YAML: `
    },
    {
        prompt: "An organization has offices across multiple locations and the technology team has configured an Application Load Balancer across targets in multiple Availability Zones. The team wants to analyze the incoming requests for latencies and the client's IP address patterns.  Which feature of the Load Balancer will help collect the required information?",
        example: "ALB access logs - Elastic Load Balancing provides access logs that capture detailed information about requests sent to your load balancer. Each log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses. You can use these access logs to analyze traffic patterns and troubleshoot issues. Access logging is an optional feature of Elastic Load Balancing that is disabled by default.",
        attachment: "./img/2023-04-21-15-02-12.png",
        description: "Incorrect options:  CloudTrail logs - Elastic Load Balancing is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in Elastic Load Balancing. CloudTrail captures all API calls for Elastic Load Balancing as events. You can use AWS CloudTrail to capture detailed information about the calls made to the Elastic Load Balancing API and store them as log files in Amazon S3. You can use these CloudTrail logs to determine which API calls were made, the source IP address where the API call came from, who made the call, when the call was made, and so on.  CloudWatch metrics - Elastic Load Balancing publishes data points to Amazon CloudWatch for your load balancers and your targets. CloudWatch enables you to retrieve statistics about those data points as an ordered set of time-series data, known as metrics. You can use metrics to verify that your system is performing as expected. This is the right feature if you wish to track a certain metric.  ALB request tracing - You can use request tracing to track HTTP requests. The load balancer adds a header with a trace identifier to each request it receives. Request tracing will not help you to analyze latency specific data."
    },
    {
        prompt: "Which of the following security credentials can only be created by the AWS Account root user? ",
        example: "CloudFront Key Pairs - IAM users can't create CloudFront key pairs. You must log in using root credentials to create key pairs.  To create signed URLs or signed cookies, you need a signer. A signer is either a trusted key group that you create in CloudFront, or an AWS account that contains a CloudFront key pair. AWS recommends that you use trusted key groups with signed URLs and signed cookies instead of using CloudFront key pairs.",
        description: "Incorrect options:  EC2 Instance Key Pairs - You use key pairs to access Amazon EC2 instances, such as when you use SSH to log in to a Linux instance. These key pairs can be created from the IAM user login and do not need root user access.  IAM User Access Keys - Access keys consist of two parts: an access key ID and a secret access key. You use access keys to sign programmatic requests that you make to AWS if you use AWS CLI commands (using the SDKs) or using AWS API operations. IAM users can create their own Access Keys, does not need root access.  IAM User passwords - Every IAM user has access to his own credentials and can reset the password whenever they need to."
    },
    {
        prompt: "A gaming company wants to store information about all the games that the company has released. Each game has a name, version number, and category (such as sports, puzzles, strategy, etc). The game information also can include additional properties about the supported platforms and technical specifications. This additional information is inconsistent across games.  You have been hired as an AWS Certified Developer Associate to build a solution that addresses the following use cases:  For a given name and version number, get all details about the game that has that name and version number.  For a given name, get all details about all games that have that name.  For a given category, get all details about all games in that category.  What will you recommend as the most efficient solution?",
        example: "Set up an Amazon DynamoDB table with a primary key that consists of the name as the partition key and the version number as the sort key. Create a global secondary index that has the category as the partition key and the name as the sort key",
        description: "When you create a DynamoDB table, in addition to the table name, you must specify the primary key of the table. The primary key uniquely identifies each item in the table, so that no two items can have the same key. You can create one or more secondary indexes on a table. A secondary index lets you query the data in the table using an alternate key, in addition to queries against the primary key. DynamoDB doesn't require that you use indexes, but they give your applications more flexibility when querying your data.\n\
        \n\n\
        Incorrect options:  Set up an Amazon DynamoDB table with a primary key that consists of the category as the partition key and the version number as the sort key. Create a global secondary index that has the name as the partition key - The DynamoDB table for this option has the primary key and GSI that do not solve for the condition - \"For a given name and version number, get all details about the game that has that name and version number\". This option does not allow for efficient querying of a specific game by its name and version number as you need multiple queries which would be less efficient than the single query allowed by the correct option.  Set up an Amazon RDS MySQL instance having a games table that contains columns for name, version number, and category. Configure the name column as the primary key - This option is not the right fit as it does not allow you to efficiently query on the version number and category columns.  Permanently store the name, version number, and category information about the games in an Amazon Elasticache for Memcached instance - You cannot use Elasticache for Memcached to permanently store values meant to be persisted in a database (relational or NoSQL). Elasticache is a caching layer. So this option is incorrect.",
        attachment: "./img/2023-04-21-15-10-37.png"
    },
    {
        prompt: "A retail company is migrating its on-premises database to Amazon RDS for PostgreSQL. The company has read-heavy workloads. The development team at the company is looking at refactoring the code to achieve optimum read performance for SQL queries.  Which solution will address this requirement with the least current as well as future development effort?",
        example: "Set up Amazon RDS with one or more read replicas. Refactor the application code so that the queries use the endpoint for the read replicas\n\
        Amazon RDS uses the PostgreSQL DB engine's built-in replication functionality to create a special type of DB instance called a read replica from a source DB instance. The source DB instance becomes the primary DB instance. Updates made to the primary DB instance are asynchronously copied to the read replica. You can reduce the load on your primary DB instance by routing read queries from your applications to the read replica. Using read replicas, you can elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the given use case, you can achieve optimum read performance for SQL queries by using the read-replica endpoint for the read-heavy workload.",
        attachment: "./img/2023-04-21-15-17-48.png",
        description: "Incorrect options:  Configure Elasticache for Redis to act as a caching layer for Amazon RDS. Refactor the application code so that the queries use the Elasticache for Redis endpoint  Configure Elasticache for Memcached to act as a caching layer for Amazon RDS. Refactor the application code so that the queries use the Elasticache for Memcached endpoint  Both Redis and Memcached are popular, open-source, in-memory data stores (also known as in-memory caches). These are not relational databases and cannot be used to run SQL queries. So, both these options are incorrect.  Set up Amazon RDS in the multi-AZ configuration with a single standby instance. Refactor the application code so that the queries use the standby instance endpoint - In an Amazon RDS Multi-AZ deployment with a single standby instance, Amazon RDS automatically creates a primary database (DB) instance and synchronously replicates the data to an instance in a different AZ. When it detects a failure, Amazon RDS automatically fails over to a standby instance without manual intervention. You cannot route the read queries from an application to the standby instance of a multi-AZ RDS database as it's not accessible for the read traffic in the single standby instance configuration."
    },
    {
        prompt: "A developer is configuring a bucket policy that denies upload object permission to any requests that do not include the x-amz-server-side-encryption header requesting server-side encryption with SSE-KMS for an Amazon S3 bucket - examplebucket.  Which of the following policies is the right fit for the given requirement?",
        description:`{ "StringNotEquals":{ "s3:x-amz-server-side-encryption":"aws:kms" } } } ] }\n\n\
        { "Version":"2012-10-17", "Id":"PutObjectPolicy", "Statement":[{ "Sid":"DenyUnEncryptedObjectUploads", "Effect":"Deny", "Principal":"", "Action":"s3:PutObject", "Resource":"arn:aws:s3:::examplebucket/", "Condition": .... `,
        example:`{ "Version":"2012-10-17", "Id":"PutObjectPolicy", "Statement":[{ "Sid":"DenyUnEncryptedObjectUploads", "Effect":"Deny", "Principal":"", "Action":"s3:PutObject", "Resource":"arn:aws:s3:::examplebucket/", "Condition":{ "StringNotEquals":{ "s3:x-amz-server-side-encryption":"aws:kms" } } } ] } `,
    },
    {
        prompt: "As an AWS Certified Developer Associate, you have configured the AWS CLI on your workstation. Your default region is us-east-1 and your IAM user has permissions to operate commands on services such as EC2, S3 and RDS in any region. You would like to execute a command to stop an EC2 instance in the us-east-2 region.  What of the following is the MOST optimal solution to address this use-case?",
        example: "Use the --region parameter: If the region parameter is not set, then the CLI command is executed against the default AWS region.  You can also review all general options for AWS CLI: ",
        attachment: "./img/2023-04-21-15-25-31.png",
        description: "Incorrect options:  You need to override the default region by using aws configure - This is not the most optimal way as you will have to change it again to reset the default region.  You should create a new IAM user just for that other region - This is not the most optimal way as you would need to manage two IAM user profiles.  Use boto3 dependency injection - With the CLI you do not use boto3. This option is a distractor."
    },
    {
        prompt: "A company uses Elastic Beanstalk to manage its IT infrastructure on AWS Cloud and it would like to deploy the new application version to the EC2 instances. When the deployment is executed, some instances should serve requests with the old application version, while other instances should serve requests using the new application version until the deployment is completed.  Which deployment meets this requirement without incurring additional costs?",
        example: "With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications. Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.  How Elastic BeanStalk Works: via - https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html  The rolling deployment policy deploys the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment's capacity by the number of instances in a batch. The cost remains the same as the number of EC2 instances does not increase. This policy avoids downtime and minimizes reduced availability, at a cost of a longer deployment time.",
        attachment: "./img/2023-04-21-15-37-18.png",
        description: "Incorrect options:  Immutable - The 'Immutable' deployment policy ensures that your new application version is always deployed to new instances, instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails.  All at once - This policy deploys the new version to all instances simultaneously. Although 'All at once' is the quickest deployment method, but the application may become unavailable to users (or have low availability) for a short time.  Rolling with additional batches - This policy deploys the new version in batches, but first launches a new batch of instances to ensure full capacity during the deployment process. This policy avoids any reduced availability, at a cost of an even longer deployment time compared to the Rolling method. Suitable if you must maintain the same bandwidth throughout the deployment. These increase the costs as you're adding extra instances during the deployment.  Reference:  https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html"
    },
    {
        prompt: "An IT company is configuring Auto Scaling for its Amazon EC2 instances spread across different AZs and Regions.  Which of the following scenarios are NOT correct about EC2 Auto Scaling? (Select two)\n\n\
        options:\n\
        ",
        example: "An IT company is configuring Auto Scaling for its Amazon EC2 instances spread across different AZs and Regions. Which of the following scenarios are NOT correct about EC2 Auto Scaling? ( Select two)  - Auto Saling groups that span across multiple Regions need to be enabled for all the Regions specified - Amazon EC2 Auto Scaling attempts to distribute instances evenly between the AZs that are enabled for your Auto Scaling group - An Auto Scaling group can contain EC2 instances in one or more AZs within the same Region - ForAuto Scaling groups in a VPC, the EC2 instances are launched in subnets - An Auto Scaling group can contain instances in only on AZ zone of a region",
        attachment: "./img/2023-04-21-15-46-04.png",
        description: "Incorrect options:  An Auto Scaling group can contain EC2 instances in one or more Availability Zones within the same Region - This is a valid statement. Auto Scaling groups can span across the availability Zones of a Region.  Amazon EC2 Auto Scaling attempts to distribute instances evenly between the Availability Zones that are enabled for your Auto Scaling group - When one Availability Zone becomes unhealthy or unavailable, Auto Scaling launches new instances in an unaffected Availability Zone. When the unhealthy Availability Zone returns to a healthy state, Auto Scaling automatically redistributes the application instances evenly across all of the designated Availability Zones.  For Auto Scaling groups in a VPC, the EC2 instances are launched in subnets - For Auto Scaling groups in a VPC, the EC2 instances are launched in subnets. Customers can select the subnets for your EC2 instances when you create or update the Auto Scaling group."
    },
    {
        prompt: "A development team lead is configuring policies for his team at an IT company. Which of the following policy types only limit permissions but cannot grant permissions (Select two)?  - AWS Organizations Service Control Policy (SCP) - Resource-based policy - Permissions boundary - Identity-based policy - Access control list (ACL)",
        example: "Correct options:  AWS Organizations Service Control Policy (SCP) – Use an AWS Organizations Service Control Policy (SCP) to define the maximum permissions for account members of an organization or organizational unit (OU). SCPs limit permissions that identity-based policies or resource-based policies grant to entities (users or roles) within the account, but do not grant permissions.  Permissions boundary - Permissions boundary is a managed policy that is used for an IAM entity (user or role). The policy defines the maximum permissions that the identity-based policies can grant to an entity, but does not grant permissions.",
        attachment: "./img/2023-04-21-15-51-47.png",
        description: "INCORRECT: Access control list (ACL) - Use ACLs to control which principals in other accounts can access the resource to which the ACL is attached. ACLs are similar to resource-based policies, although they are the only policy type that does not use the JSON policy document structure. ACLs are cross-account permissions policies that grant permissions to the specified principal.  Resource-based policy - Resource-based policies grant permissions to the principal that is specified in the policy. Principals can be in the same account as the resource or in other accounts. The most common examples of resource-based policies are Amazon S3 bucket policies and IAM role trust policies.  Identity-based policy - Help attach managed and inline policies to IAM identities (users, groups to which users belong, or roles). Identity-based policies grant permissions to an identity."
    },
    {
        prompt: "You have chosen AWS Elastic Beanstalk to upload your application code and allow it to handle details such as provisioning resources and monitoring.  When creating configuration files for AWS Elastic Beanstalk which naming convention should you follow?\n\
        for config | <mysettings> and ebexetensions",
        example: ".ebextensions/<mysettings>.config : You can add AWS Elastic Beanstalk configuration files (.ebextensions) to your web application's source code to configure your environment and customize the AWS resources that it contains. Configuration files are YAML or JSON formatted documents with a .config file extension that you place in a folder named .ebextensions and deploy in your application source bundle.",
        attachment: "./img/2023-04-21-15-56-45.png"
    },
    {
        prompt: "You are a developer for a web application written in .NET which uses the AWS SDK. You need to implement an authentication mechanism that returns a JWT (JSON Web Token).  Which AWS service will help you with token handling and management?",
        example: `Correct option:  "Cognito User Pools"  After successful authentication, Amazon Cognito returns user pool tokens to your app. You can use the tokens to grant your users access to your own server-side resources, or to the Amazon API Gateway.  Amazon Cognito user pools implement ID, access, and refresh tokens as defined by the OpenID Connect (OIDC) open standard.  The ID token is a JSON Web Token (JWT) that contains claims about the identity of the authenticated user such as name, email, and phone_number. You can use this identity information inside your application. The ID token can also be used to authenticate users against your resource servers or server applications.`,
        attachment: "./img/2023-04-21-16-00-28.png",
        description: `Incorrect options:  "API Gateway" - If you are processing tokens server-side and using other programming languages not supported in AWS it may be a good choice. Other than that, go with a service already providing the functionality.  "Cognito Identity Pools" - You can use Identity pools to grant your users access to other AWS services. With an identity pool, your users can obtain temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB. Identity pools support anonymous guest users, as well as the specific identity providers that you can use to authenticate users for identity pools.  "Cognito Sync" - Amazon Cognito Sync is an AWS service and client library that enables cross-device syncing of application-related user data. You can use it to synchronize user profile data across mobile devices and the web without requiring your own backend.`
    },
    {
        prompt: "Whats the difference between User and Identity Pools? Explain the use cases for each.",
        description: "User Pools are used to manage user authentication and authorization, and they provide features such as user sign-up, sign-in, and password recovery. User Pools are used to authenticate and authorize individual users, and they can be used to integrate with social identity providers like Facebook, Google, and Amazon, as well as enterprise identity providers using SAML 2.0.  Identity Pools, on the other hand, provide a way to authorize and authenticate access to AWS resources for groups of users, known as federated identities. Identity Pools allow users to authenticate with third-party identity providers, such as Facebook or Google, and obtain temporary AWS credentials that grant access to AWS resources. These credentials are managed by AWS Security Token Service (STS) and can be customized to restrict access to specific AWS resources or services.",
        example: "You can use User pool for developing an app where users can sign up, while Identity Pool is for your applications to be able to authenticate access to other aws resources"
    },
    {
        prompt: "A development team wants to build an application using serverless architecture. The team plans to use AWS Lambda functions extensively to achieve this goal. The developers of the team work on different programming languages like Python, .NET and Javascript. The team wants to model the cloud infrastructure using any of these programming languages.  Which AWS service/tool should the team use for the given use-case?",
        example: "AWS Cloud Development Kit (CDK) - The AWS Cloud Development Kit (AWS CDK) is an open-source software development framework to define your cloud application resources using familiar programming languages.  Provisioning cloud applications can be a challenging process that requires you to perform manual actions, write custom scripts, maintain templates, or learn domain-specific languages. AWS CDK uses the familiarity and expressive power of programming languages such as JavaScript/TypeScript, Python, Java, and .NET for modeling your applications. It provides you with high-level components called constructs that preconfigure cloud resources with proven defaults, so you can build cloud applications without needing to be an expert. AWS CDK provisions your resources in a safe, repeatable manner through AWS CloudFormation. It also enables you to compose and share your own custom constructs that incorporate your organization's requirements, helping you start new projects faster.",
        description: "Incorrect options:  AWS CloudFormation - When AWS CDK applications are run, they compile down to fully formed CloudFormation JSON/YAML templates that are then submitted to the CloudFormation service for provisioning. Because the AWS CDK leverages CloudFormation, you still enjoy all the benefits CloudFormation provides such as safe deployment, automatic rollback, and drift detection. But, CloudFormation by itself is not sufficient for the current use case.  AWS Serverless Application Model (SAM) - The AWS Serverless Application Repository is a managed repository for serverless applications. It enables teams, organizations, and individual developers to store and share reusable applications, and easily assemble and deploy serverless architectures in powerful new ways. Using the Serverless Application Repository, you don't need to clone, build, package, or publish source code to AWS before deploying it.  AWS Serverless Application Model and AWS CDK both abstract AWS infrastructure as code making it easier for you to define your cloud infrastructure. If you prefer defining your serverless infrastructure in concise declarative templates, SAM is the better fit. If you want to define your AWS infrastructure in a familiar programming language, as is the requirement in the current use case, AWS CDK is the right fit.  AWS CodeDeploy - AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during application deployment, and handles the complexity of updating your applications. CodeDeploy can be used with AWS CDK for deployments."
    },
    {
        prompt: "You are creating a Cloud Formation template to deploy your CMS application running on an EC2 instance within your AWS account. Since the application will be deployed across multiple regions, you need to create a map of all the possible values for the base AMI.  How will you invoke the !FindInMap function to fulfill this use case?",
        description: `Mappings:\n RegionMap:\n us-east-1:\n HVM64: "ami-0ff8a91507f77f867"\n HVMG2: "ami-0a584ac55a7631c0c"\n us-west-1:\n HVM64: "ami-0bdb828fd58c52235"\n HVMG2: "ami-066ee5fd4a9ef77f1"\n eu-west-1:\n HVM64: "ami-047bb4163c506cd98"\n HVMG2: "ami-31c2f645"\n ap-southeast-1:\n HVM64: "ami-08569b978cc4dfa10"\n HVMG2: "ami-0be9df32ae9f92309"\n ap-northeast-1:\n HVM64: "ami-06cd52961ce9f0d85"\n HVMG2: "ami-053cdd503598e4a9d"\nResources:\n myEC2Instance:\n Type: "AWS::EC2::Instance"\n Properties:\n ImageId: !FindInMap\n - RegionMap\n - !Ref 'AWS::Region'\n - HVM64\n InstanceType: m1.small",`,
        example: "!FindInMap [ MapName, TopLevelKey, SecondLevelKey ] - The intrinsic function Fn::FindInMap returns the value corresponding to keys in a two-level map that is declared in the Mappings section. YAML Syntax for the full function name: Fn::FindInMap: [ MapName, TopLevelKey, SecondLevelKey ]  Short form of the above syntax is : !FindInMap [ MapName, TopLevelKey, SecondLevelKey ]  Where,  MapName - Is the logical name of a mapping declared in the Mappings section that contains the keys and values. TopLevelKey - The top-level key name. Its value is a list of key-value pairs. SecondLevelKey - The second-level key name, which is set to one of the keys from the list assigned to TopLevelKey."
    },
    {
        prompt: "A development team wants to deploy an AWS Lambda function that requires significant CPU utilization.  As a Developer Associate, which of the following would you suggest for reducing the average runtime of the function?",
        example: "Deploy the function with its memory allocation set to the maximum amount - Lambda allocates CPU power in proportion to the amount of memory configured. Memory is the amount of memory available to your Lambda function at runtime. You can increase or decrease the memory and CPU power allocated to your function using the Memory (MB) setting. To configure the memory for your function, set a value between 128 MB and 10,240 MB in 1-MB increments. At 1,769 MB, a function has the equivalent of one vCPU (one vCPU-second of credits per second).",
        attachment: "./img/2023-04-21-16-32-41.png",
        description: "Incorrect options:  Deploy the function into multiple AWS Regions - Deploying the Lambda function to multiple AWS Regions does not increase the compute capacity or CPU utilization capacity of Lambda. So, this option is irrelevant.  Deploy the function using Lambda layers - A Lambda layer is a .zip file archive that can contain additional code or data. A layer can contain libraries, a custom runtime, data, or configuration files. Layers promote code sharing and separation of responsibilities so that you can iterate faster on writing business logic. Layers do not increase the computational capacity of Lambda.  Deploy the function with its CPU allocation set to the maximum amount - This statement is given as a distractor. CPU allocation is an invalid parameter. As discussed above, the CPU is allocated in proportion to the memory allocated to the function."

    },
    {
        prompt: "Amazon Simple Queue Service (SQS) has a set of APIs for various actions supported by the service.  As a developer associate, which of the following would you identify as correct regarding the CreateQueue API? (Select two):\n\
        The dead-letter queue of a FIFO queue must also be a FIFO queue. Whereas, the dead-letter queue of a standard queue can be a standard queue or a FIFO queue - \n\
        You can't change the queue type after you create it \n\
        Queue tags are case insensitive. A new tag with a key identical to that of an existing tag overwrites the existing tag \n\
        The visibility timeout value for the queue is in seconds, which defaults to 30 seconds\n\
        The length of time, in seconds, for which the delivery of all messages in the queue is delayed is configured using MessageRetentionPeriod attribute",
        example: "Correct options:  You can't change the queue type after you create it - You can't change the queue type after you create it and you can't convert an existing standard queue into a FIFO queue. You must either create a new FIFO queue for your application or delete your existing standard queue and recreate it as a FIFO queue.  The visibility timeout value for the queue is in seconds, which defaults to 30 seconds - The visibility timeout for the queue is in seconds. Valid values are: An integer from 0 to 43,200 (12 hours), the Default value is 30.",
        description: "Incorrect options:  The dead-letter queue of a FIFO queue must also be a FIFO queue. Whereas, the dead-letter queue of a standard queue can be a standard queue or a FIFO queue - The dead-letter queue of a FIFO queue must also be a FIFO queue. Similarly, the dead-letter queue of a standard queue must also be a standard queue.  The length of time, in seconds, for which the delivery of all messages in the queue is delayed is configured using MessageRetentionPeriod attribute - The length of time, in seconds, for which the delivery of all messages in the queue is delayed is configured using DelaySeconds attribute. MessageRetentionPeriod attribute controls the length of time, in seconds, for which Amazon SQS retains a message.  Queue tags are case insensitive. A new tag with a key identical to that of an existing tag overwrites the existing tag - Queue tags are case-sensitive. A new tag with a key identical to that of an existing tag overwrites the existing tag. To be able to tag a queue on creation, you must have the sqs:CreateQueue and sqs:TagQueue permissions."

    },
    {
        prompt: "A company wants to improve the performance of its popular API service that offers unauthenticated read access to daily updated statistical information via Amazon API Gateway and AWS Lambda.\n\
        What measures can the company take?",
        example: "Enable API caching in API Gateway  API Gateway provides a few strategies for optimizing your API to improve responsiveness, like response caching and payload compression. You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API.",
        description: "Incorrect options:  Set up usage plans and API keys in API Gateway - After you create, test, and deploy your APIs, you can use API Gateway usage plans to make them available as product offerings for your customers. You can configure usage plans and API keys to allow customers to access selected APIs, and begin throttling requests to those APIs based on defined limits and quotas. These can be set at the API, or API method level. This option is incorrect as usage plans and API keys cannot be used to improve the responsiveness of the API.  Configure API Gateway to use Elasticache for Memcached - This option has been added as a distractor. Elasticache for Memcached cannot be used with API Gateway to improve the responsiveness of the API for the given use case. You should note that Elasticache for Memcached is a downstream service in the request flow.  Configure API Gateway to use Gateway VPC Endpoint - This option has been added as a distractor. Gateway endpoints provide reliable connectivity to Amazon S3 and DynamoDB without requiring an internet gateway or a NAT device for your VPC. Gateway endpoints do not enable AWS PrivateLink."
    },
    {
        prompt: "A cybersecurity firm wants to run their applications on single-tenant hardware to meet security guidelines.  Which of the following is the MOST cost-effective way of isolating their Amazon EC2 instances to a single tenant?\n\
        Dedicated Hosts, On Demand Instances, Spot Instances, or Dedicated Instances?",
        example: "Dedicated Instances - Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer. Dedicated Instances that belong to different AWS accounts are physically isolated at a hardware level, even if those accounts are linked to a single-payer account. However, Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated Instances.  A Dedicated Host is also a physical server that's dedicated for your use. With a Dedicated Host, you have visibility and control over how instances are placed on the server.  Differences between Dedicated Hosts and Dedicated Instances: ",
        attachment: "./img/2023-04-21-16-47-42.png",
        description: "Incorrect options:  Spot Instances - A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price. Any instance present with unused capacity will be allocated. Even though this is cost-effective, it does not fulfill the single-tenant hardware requirement of the client and hence is not the correct option.  Dedicated Hosts - An Amazon EC2 Dedicated Host is a physical server with EC2 instance capacity fully dedicated to your use. Dedicated Hosts allow you to use your existing software licenses on EC2 instances. With a Dedicated Host, you have visibility and control over how instances are placed on the server. This option is costlier than the Dedicated Instance and hence is not the right choice for the current requirement.  On-Demand Instances - With On-Demand Instances, you pay for compute capacity by the second with no long-term commitments. You have full control over its lifecycle—you decide when to launch, stop, hibernate, start, reboot, or terminate it. Hardware isolation is not possible and on-demand has one of the costliest instance charges and hence is not the correct answer for current requirements."
    },
    {
        prompt: "You have created a Java application that uses RDS for its main data storage and ElastiCache for user session storage. The application needs to be deployed using Elastic Beanstalk and every new deployment should allow the application servers to reuse the RDS database. On the other hand, user session data stored in ElastiCache can be re-created for every deployment.  Which of the following configurations will allow you to achieve this? (Select two)",
        example: "ElastiCache defined in .ebextensions/ - Any resources created as part of your .ebextensions is part of your Elastic Beanstalk template and will get deleted if the environment is terminated.  via - https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html  RDS database defined externally and referenced through environment variables - To decouple your database instance from your environment, you can run a database instance in Amazon RDS and configure your application to connect to it on launch. This enables you to connect multiple environments to a database, terminate an environment without affecting the database, and perform seamless updates with blue-green deployments. To allow the Amazon EC2 instances in your environment to connect to an outside database, you can configure the environment's Auto Scaling group with an additional security group.  Using Elastic Beanstalk with",
        attachment: ".u/img/2023-04-21-16-54-40.png",
    },
    {
        prompt: "An organization has hosted its EC2 instances in two AZs. AZ1 has two instances and AZ2 has 8 instances. The Elastic Load Balancer managing the instances in the two AZs has cross-zone load balancing enabled in its configuration.  What percentage traffic will each of the instances in AZ1 receive?",
        example: "A load balancer accepts incoming traffic from clients and routes requests to its registered targets (such as EC2 instances) in one or more Availability Zones.  The nodes for a load balancer distribute requests from clients to registered targets. When cross-zone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones. When cross-zone load balancing is disabled, each load balancer node distributes traffic only across the registered targets in its Availability Zone. With Application Load Balancers, cross-zone load balancing is always enabled.  10 - When cross-zone load balancing is enabled, each of the 10 targets receives 10% of the traffic. This is because each load balancer node can route its 50% of the client traffic to all 10 targets (present in both AZs).",
        attachment: "./img/2023-04-21-20-41-49.png",
        description: "INCORRECT: 25 - If cross-zone load balancing is disabled, each of the two targets in AZ1 will receive 25% of the traffic. Because the load balancer is only able to send to the targets registered in AZ1 (AZ2 instances are not accessible for load balancer on AZ1)  20 - Invalid option, given only as a distractor.  15 - Invalid option, given only as a distractor."
    },
    {
        prompt: "ECS Fargate container tasks are usually spread across Availability Zones (AZs) and the underlying workloads need persistent cross-AZ shared access to the data volumes configured for the container tasks.  Which of the following solutions is the best choice for these workloads?",
        example: "Amazon EFS volumes - EFS volumes provide a simple, scalable, and persistent file storage for use with your Amazon ECS tasks. With Amazon EFS, storage capacity is elastic, growing and shrinking automatically as you add and remove files. Your applications can have the storage they need, when they need it. Amazon EFS volumes are supported for tasks hosted on Fargate or Amazon EC2 instances.  You can use Amazon EFS file systems with Amazon ECS to export file system data across your fleet of container instances. That way, your tasks have access to the same persistent storage, no matter the instance on which they land. However, you must configure your container instance AMI to mount the Amazon EFS file system before the Docker daemon starts. Also, your task definitions must reference volume mounts on the container instance to use the file system.",
        description: "INCORRECT: Docker volumes - A Docker-managed volume that is created under /var/lib/docker/volumes on the host Amazon EC2 instance. Docker volume drivers (also referred to as plugins) are used to integrate the volumes with external storage systems, such as Amazon EBS. The built-in local volume driver or a third-party volume driver can be used. Docker volumes are only supported when running tasks on Amazon EC2 instances.  Bind mounts - A file or directory on the host, such as an Amazon EC2 instance or AWS Fargate, is mounted into a container. Bind mount host volumes are supported for tasks hosted on Fargate or Amazon EC2 instances. Bind mounts provide temporary storage, and hence these are a wrong choice for this use case.  AWS Storage Gateway volumes - This is an incorrect choice, given only as a distractor.",

    },
    {
        prompt: "When running a Rolling deployment in Elastic Beanstalk environment, only two batches completed the deployment successfully, while rest of the batches failed to deploy the updated version. Following this, the development team terminated the instances from the failed deployment.  What will be the status of these failed instances post termination?\n\n\
        Elastic Beanstalk will not replace the failed instances     Elastic Beanstalk will replace the failed instances with instances running the application version from the oldest successful deployment     Elastic Beanstalk will replace the failed instances with instances running the application version from the most recent successful deployment (Correct)     Elastic Beanstalk will replace the failed instances after the application version to be installed is manually chosen from AWS Console",
        example: "Elastic Beanstalk will replace them with instances running the application version from the most recent successful deployment      When processing a batch, Elastic Beanstalk detaches all instances in the batch from the load balancer, deploys the new application version, and then reattaches the instances. If you enable connection draining, Elastic Beanstalk drains existing connections from the Amazon EC2 instances in each batch before beginning the deployment.  If a deployment fails after one or more batches completed successfully, the completed batches run the new version of your application while any pending batches continue to run the old version. You can identify the version running on the instances in your environment on the health page in the console. This page displays the deployment ID of the most recent deployment that was executed on each instance in your environment. If you terminate instances from the failed deployment, Elastic Beanstalk replaces them with instances running the application version from the most recent successful deployment.",
        description: "Incorrect options:  Elastic Beanstalk will not replace the failed instances  Elastic Beanstalk will replace the failed instances with instances running the application version from the oldest successful deployment  Elastic Beanstalk will replace the failed instances after the application version to be installed is manually chosen from AWS Console"
    },
    {
        prompt: "Which of the following best describes how KMS Encryption works?\n\
        KMS stores the CMK, and receives data from the clients, which it encrypts and sends back\n\
        KMS receives CMK from the client at every encrypt call, and encrypts the data with that\n\
        KMS sends the CMK to the client, which performs the encryption and then deletes the CMK\n\
        KMS generates a new CMK for each Encrypt call and encrypts the data with it ",
        example: "KMS stores the CMK, and receives data from the clients, which it encrypts and sends back   A customer master key (CMK) is a logical representation of a master key. The CMK includes metadata, such as the key ID, creation date, description, and key state. The CMK also contains the key material used to encrypt and decrypt data. You can generate CMKs in KMS, in an AWS CloudHSM cluster, or import them from your key management infrastructure.  AWS KMS supports symmetric and asymmetric CMKs. A symmetric CMK represents a 256-bit key that is used for encryption and decryption. An asymmetric CMK represents an RSA key pair that is used for encryption and decryption or signing and verification (but not both), or an elliptic curve (ECC) key pair that is used for signing and verification.  AWS KMS supports three types of CMKs: customer-managed CMKs, AWS managed CMKs, and AWS owned CMKs.",
        description: "Incorrect options:  KMS receives CMK from the client at every encrypt call, and encrypts the data with that - You can import your own CMK (Customer Master Key) but it is done once and then you can encrypt/decrypt as needed.  KMS sends the CMK to the client, which performs the encryption and then deletes the CMK - KMS does not send CMK to the client, KMS itself encrypts, and then decrypts the data.  KMS generates a new CMK for each Encrypt call and encrypts the data with it - KMS does not generate a new key each time but you can have KMS rotate the keys for you. Best practices discourage extensive reuse of encryption keys so it is good practice to generate new keys.",
        attachment: "./img/2023-04-21-20-50-11.png"
    },
    {
        prompt: "A multi-national company has multiple business units with each unit having its own AWS account. The development team at the company would like to debug and trace data across accounts and visualize it in a centralized account.  As a Developer Associate, which of the following solutions would you suggest for the given use-case?\n\n\
        VPC Flow Logs     CloudWatch Events     CloudTrail     X-Ray (Correct)",
        example: "X-Ray  AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components.  You can use X-Ray to collect data across AWS Accounts. The X-Ray agent can assume a role to publish data into an account different from the one in which it is running. This enables you to publish data from various components of your application into a central account.",
        attachment: "./img/2023-04-21-20-57-33.png",
        descrription: "Incorrect options: VPC Flow Logs: VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data is used to analyze network traces and helps with network security. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3. You cannot use VPC Flow Logs to debug and trace data across accounts.  CloudWatch Events: Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. These help to trigger notifications based on changes happening in AWS services. You cannot use CloudWatch Events to debug and trace data across accounts.  CloudTrail: With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. You can use AWS CloudTrail to answer questions such as - “Who made an API call to modify this resource?”. CloudTrail provides event history of your AWS account activity thereby enabling governance, compliance, operational auditing, and risk auditing of your AWS account. You cannot use CloudTrail to debug and trace data across accounts."

    }



]


module.exports = {
    aws_certification_associate_developer
}